{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (KNN: Regression)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9de1c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, \\\n",
    "cross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Ridge, Lasso\n",
    "from scipy.stats import uniform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./datasets/train_regression.csv')\n",
    "test = pd.read_csv('./datasets/test_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "## 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc06dc",
   "metadata": {},
   "source": [
    "Put the data pre-processing code. You don't need to explain it. You may use the same code from last quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9dfc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize the property types\n",
    "def categorize_property(property_type):\n",
    "    if 'Entire' in property_type:\n",
    "        return 'Entire Home/Apartment'\n",
    "    elif 'Private' in property_type:\n",
    "        return 'Private Room'\n",
    "    elif 'Shared' in property_type:\n",
    "        return 'Shared Accommodation'\n",
    "    elif property_type in ['Room in hotel', 'Room in boutique hotel', 'Boat']:\n",
    "        return 'Specialty Accommodations'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51b4ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall function to clean training and test data\n",
    "def clean_data(df):\n",
    "    \n",
    "    # Remove $ from response variable and convert to float in training data\n",
    "    if 'price' in df.columns:\n",
    "        df.price = df.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "    # replace missing values of numeric variables wtih the median\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # replace missing values of categorical variables with the mode \n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n",
    "    \n",
    "    # log transform response variable for training data and drop price\n",
    "    if 'price' in df.columns:\n",
    "        df['log_price'] = np.log(df['price'])\n",
    "    \n",
    "    # replace any 0 values to 1 so that it can go through log transformation\n",
    "    df['beds'] = df['beds'].replace(0, .01)\n",
    "    df['accommodates'] = df['accommodates'].replace(0, .01)\n",
    "    df['number_of_reviews'] = df['number_of_reviews'].replace(0, .01)\n",
    "    df['reviews_per_month'] = df['reviews_per_month'].replace(0, .01)\n",
    "    df['number_of_reviews_ltm'] = df['number_of_reviews_ltm'].replace(0, .01)\n",
    "    df['number_of_reviews_l30d'] = df['number_of_reviews_l30d'].replace(0, .01)\n",
    "    df['host_total_listings_count'] = df['host_total_listings_count'].replace(0, .01)\n",
    "    df['host_listings_count'] = df['host_listings_count'].replace(0, .01)\n",
    "    df['calculated_host_listings_count_private_rooms'] = df['calculated_host_listings_count_private_rooms'].replace(0, .01)\n",
    "    df['calculated_host_listings_count_shared_rooms'] = df['calculated_host_listings_count_shared_rooms'].replace(0, .01)\n",
    "    df['calculated_host_listings_count_entire_homes'] = df['calculated_host_listings_count_entire_homes'].replace(0, .01)\n",
    "    \n",
    "    df['log_beds'] = np.log(df.beds)\n",
    "    df['log_accommodates'] = np.log(df.accommodates)\n",
    "    df['log_reviews'] = np.log(df.number_of_reviews)\n",
    "    df['log_reviews_per_month'] = np.log(df.reviews_per_month)\n",
    "    df['log_reviews_ltm'] = np.log(df.number_of_reviews_ltm)\n",
    "    df['log_reviews_l30d'] = np.log(df.number_of_reviews_l30d)\n",
    "    df['log_host_total_listings_count'] = np.log(df.host_total_listings_count)\n",
    "    df['log_host_listings_count'] = np.log(df.host_listings_count)\n",
    "    df['log_host_listings_count_private_rooms'] = np.log(df.calculated_host_listings_count_private_rooms)\n",
    "    df['log_host_listings_count_shared_rooms'] = np.log(df.calculated_host_listings_count_shared_rooms)\n",
    "    df['log_host_listings_count_entire_homes'] = np.log(df.calculated_host_listings_count_entire_homes)\n",
    "\n",
    "    # calculate the number of days since the host became a host\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "    current_date = dt.now()\n",
    "    df['host_since_days'] = (current_date - df['host_since']).dt.days\n",
    "    \n",
    "    # calculate days since first/last review\n",
    "    df['first_review'] = pd.to_datetime(df['first_review'], errors='coerce')\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "\n",
    "    df['first_review_days'] = (current_date - df['first_review']).dt.days\n",
    "    df['last_review_days'] = (current_date - df['last_review']).dt.days\n",
    "    \n",
    "    # make response_rate and acceptance_rate into numeric dtype\n",
    "    df['host_response_rate'] = df['host_response_rate'].str.rstrip('%').astype('float')\n",
    "    df['host_acceptance_rate'] = df['host_acceptance_rate'].str.rstrip('%').astype('float')\n",
    "    \n",
    "    # subgroup property_type (similar levels as room_type so discard room predictor)\n",
    "    df['property_cats'] = df['property_type'].apply(categorize_property)\n",
    "    \n",
    "    # extract numeric values from the 'bathrooms' column\n",
    "    df['bath_numeric'] = df['bathrooms_text'].str.extract('(\\d+\\.*\\d*)', expand=False).astype(float)\n",
    "\n",
    "    # handle \"Half-bath\" by assigning a numeric value of 0.5\n",
    "    df['bath_numeric'] = df.apply(lambda row: 0.5 if 'half' in row['bathrooms_text'].lower() \\\n",
    "                                  else row['bath_numeric'], axis=1)\n",
    "    \n",
    "    # to binary\n",
    "    df.host_is_superhost = df.host_is_superhost.replace({'t': 1, 'f': 0})\n",
    "    df.host_identity_verified = df.host_identity_verified.replace({'t': 1, 'f': 0})\n",
    "    df.host_has_profile_pic = df.host_has_profile_pic.replace({'t': 1, 'f': 0})\n",
    "    df.has_availability = df.has_availability.replace({'t': 1, 'f': 0})\n",
    "    df.instant_bookable = df.instant_bookable.replace({'t': 1, 'f': 0})\n",
    "\n",
    "    # drop the modified/redundant columns\n",
    "    df.drop(columns = ['host_since', 'first_review', 'last_review', 'property_type', 'bathrooms_text', \\\n",
    "                       'number_of_reviews', 'reviews_per_month', 'number_of_reviews_ltm', \\\n",
    "                       'number_of_reviews_l30d', 'host_total_listings_count', 'host_listings_count', \\\n",
    "                      'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', \\\n",
    "                       'calculated_host_listings_count_entire_homes', 'host_id'], inplace = True)\n",
    "    \n",
    "    # drop predictors that have low corr with log_price and high corr with others to help remove multi-collinearity\n",
    "    df.drop(columns = ['first_review_days', 'last_review_days', 'host_acceptance_rate', 'host_response_rate', \n",
    "                       'availability_60', 'availability_90', 'minimum_minimum_nights', \\\n",
    "                       'maximum_maximum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', \\\n",
    "                       'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e83356cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(train)\n",
    "clean_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3d2e868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract mean price of each host_location predictor\n",
    "host_loc_avg_prices = train.groupby('host_location')['price'].mean().reset_index()\n",
    "host_loc_avg_prices['avg_host_location_price'] = host_loc_avg_prices['price']\n",
    "train = pd.merge(train, host_loc_avg_prices[['host_location', 'avg_host_location_price']], on='host_location', how='left')\n",
    "\n",
    "# extract mean price of each host_neighbourhood predictor\n",
    "host_neigh_avg_prices = train.groupby('host_neighbourhood')['price'].mean().reset_index()\n",
    "host_neigh_avg_prices['avg_host_neighbourhood_price'] = host_neigh_avg_prices['price']\n",
    "train = pd.merge(train, host_neigh_avg_prices[['host_neighbourhood', 'avg_host_neighbourhood_price']], on='host_neighbourhood', how='left')\n",
    "\n",
    "# extract mean price of each neighbourhood_cleansed predictor\n",
    "neigh_avg_prices = train.groupby('neighbourhood_cleansed')['price'].mean().reset_index()\n",
    "neigh_avg_prices['avg_neighbourhood_price'] = neigh_avg_prices['price']\n",
    "train = pd.merge(train, neigh_avg_prices[['neighbourhood_cleansed', 'avg_neighbourhood_price']], on='neighbourhood_cleansed', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "10baded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract mean price of each host_location predictor for test data\n",
    "test = pd.merge(test, host_loc_avg_prices[['host_location', 'avg_host_location_price']], on='host_location', how='left')\n",
    "\n",
    "# extract mean price of each host_neighbourhood predictor for test data\n",
    "test = pd.merge(test, host_neigh_avg_prices[['host_neighbourhood', 'avg_host_neighbourhood_price']], on='host_neighbourhood', how='left')\n",
    "\n",
    "# extract mean price of each neighbourhood_cleansed predictor for test data\n",
    "test = pd.merge(test, neigh_avg_prices[['neighbourhood_cleansed', 'avg_neighbourhood_price']], on='neighbourhood_cleansed', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7c62919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the categorical predictors that we used right above\n",
    "train = train.drop(columns = ['host_neighbourhood', 'neighbourhood_cleansed', 'host_location'])\n",
    "test = test.drop(columns = ['host_neighbourhood', 'neighbourhood_cleansed', 'host_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d1cbbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out extreme outliers\n",
    "train = train[train.price < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "74d8d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the most influential point\n",
    "train = train.drop(index = 2850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "77d1aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE the remaining categorical variables\n",
    "host_response_time_dummies = pd.get_dummies(train['host_response_time'], prefix='host_response_time')\n",
    "train = pd.concat([train, host_response_time_dummies], axis = 1)\n",
    "\n",
    "host_response_time_dummies = pd.get_dummies(test['host_response_time'], prefix='host_response_time')\n",
    "test = pd.concat([test, host_response_time_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "459c3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_verifications_dummies = pd.get_dummies(train['host_verifications'], prefix='host_verifications')\n",
    "train = pd.concat([train, host_verifications_dummies], axis = 1)\n",
    "\n",
    "host_verifications_dummies = pd.get_dummies(test['host_verifications'], prefix='host_verifications')\n",
    "test = pd.concat([test, host_verifications_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ab65cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_type_dummies = pd.get_dummies(train['room_type'], prefix='room_type')\n",
    "train = pd.concat([train, room_type_dummies], axis = 1)\n",
    "\n",
    "room_type_dummies = pd.get_dummies(test['room_type'], prefix='room_type')\n",
    "test = pd.concat([test, room_type_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "71452fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_cats_dummies = pd.get_dummies(train['property_cats'], prefix='property_cats')\n",
    "train = pd.concat([train, property_cats_dummies], axis = 1)\n",
    "\n",
    "property_cats_dummies = pd.get_dummies(test['property_cats'], prefix='property_cats')\n",
    "test = pd.concat([test, property_cats_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f0ff24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns = ['host_response_time', 'host_verifications', 'room_type', 'property_cats'])\n",
    "test = test.drop(columns = ['host_response_time', 'host_verifications', 'room_type', 'property_cats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "04303475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable spacing\n",
    "train.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)\n",
    "test.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f990534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values of numeric variables with mean in test from added predictors\n",
    "numeric_columns = test.select_dtypes(include=['number']).columns\n",
    "test[numeric_columns] = test[numeric_columns].apply(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fc26e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set response and predictors for scaling\n",
    "y_train = train.log_price\n",
    "X_train = train.drop(columns = ['log_price', 'price', 'id'])\n",
    "X_test = test.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "706da9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly features\n",
    "poly = PolynomialFeatures(degree = 3, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3e7a421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_scaled = scaler.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33d51",
   "metadata": {},
   "source": [
    "It took me around 10 attempts to tune the model hyperparamters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": [
    "I used Randomized Search to tune my model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": [
    "A challenge I faced while tuning the hyperparamters was figuring out the optimal range to test `n_neighbors` and `r` features selected. I mainly just used trial and error to address this, as I saw the `n_neighbors` and printed out `r` feature results each run. I also had to choose a variable selection method first before running KNN, as unncessary predictors would negatively incluence KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": [
    "The code usually took 30 minutes or less each run. In total, I spent around 2-3 hours on hyperparamter tuning in addition to the time it took to implement Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfd972",
   "metadata": {},
   "source": [
    "### Variable Selection Step: Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "**Paste the hyperparameter tuning code below. You must show at least one hyperparameter tuning procedure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6234884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames with polynomial features\n",
    "X_train_poly_df = pd.DataFrame(X_train_scaled, columns=poly.get_feature_names_out(X_train.columns))\n",
    "X_test_poly_df = pd.DataFrame(X_test_scaled, columns=poly.get_feature_names_out(X_test.columns))\n",
    "\n",
    "selected_coeffs = []\n",
    "r = 0\n",
    "alphas = np.logspace(-1, 3, 200)\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha = alpha)\n",
    "    lasso.fit(X_train_scaled,y_train)\n",
    "    if ((lasso.coef_ == 0).sum() > r) & (len(selected_coeffs) <= r) :\n",
    "        selected_coeffs.append(np.where(lasso.coef_!=0)[0])\n",
    "        r = r + 1\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "grid = {'n_neighbors': np.arange(1, 41), 'weights': ['uniform', 'distance'], 'metric': ['manhattan', 'euclidean', 'minkowski']}\n",
    "\n",
    "results = []\n",
    "\n",
    "for r in range(1, 31):\n",
    "    \n",
    "    gcv = RandomizedSearchCV(model, grid, cv = kfold, n_iter = 180, random_state = 10,\n",
    "                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n",
    "    \n",
    "    selected_Xs = X_train_poly_df.iloc[:,selected_coeffs[r-1]]\n",
    "    selected_predictors = selected_Xs.columns.tolist()\n",
    "    \n",
    "    if selected_Xs.shape[1] > 0:\n",
    "        gcv.fit(selected_Xs, y_train)\n",
    "        cv_rmse = np.sqrt(-gcv.best_score_)\n",
    "        results.append({'r': r,\n",
    "                    'selected_predictors': selected_predictors,\n",
    "                    'best_params': gcv.best_params_,\n",
    "                    'cv_rmse': cv_rmse})\n",
    "        \n",
    "    else:\n",
    "        print(\"No features selected for r =\", r)\n",
    "        \n",
    "optimal_r = min(results, key = lambda x: x['cv_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d457eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract selected predictors column names\n",
    "selected_predictors = optimal_r['selected_predictors']\n",
    "\n",
    "# Find indices of selected predictor column names\n",
    "selected_predictor_indices = [X_train_poly_df.columns.get_loc(col) for col in selected_predictors]\n",
    "\n",
    "# Extract selected predictor column names\n",
    "selected_predictor_names = X_train_poly_df.columns[selected_predictor_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc357d",
   "metadata": {},
   "source": [
    "**Paste the optimal hyperparameter values below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3ad083fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': 'distance', 'n_neighbors': 20, 'metric': 'manhattan'}\n"
     ]
    }
   ],
   "source": [
    "print(optimal_r['best_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a37864",
   "metadata": {},
   "source": [
    "Using the optimal model hyperparameters, train the model, and paste the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a6462944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(metric=&#x27;manhattan&#x27;, n_neighbors=20, weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(metric=&#x27;manhattan&#x27;, n_neighbors=20, weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsRegressor(metric='manhattan', n_neighbors=20, weights='distance')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsRegressor(**optimal_r['best_params'])\n",
    "model.fit(X_train_poly_df[selected_predictor_names], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## 4) Put any ad-hoc steps for further improving model accuracy\n",
    "For example, scaling up or scaling down the predictions, capping predictions, etc.\n",
    "\n",
    "Put code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2d651",
   "metadata": {},
   "source": [
    "No further ad-hoc steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## 5) Export the predictions in the format required to submit on Kaggle\n",
    "Put code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a4d9d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test_poly_df[selected_predictor_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.exp(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "526c1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = test.id.values\n",
    "predicted = pred\n",
    "submission = pd.DataFrame({'id': id, 'predicted': predicted})\n",
    "submission = submission.reset_index(drop=True)\n",
    "submission.to_csv('regression_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
