{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2d08f12d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction problems: Report\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceeb191",
   "metadata": {},
   "source": [
    "# A) Prediction problem: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1608757",
   "metadata": {},
   "source": [
    "## A.1) Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6835689",
   "metadata": {},
   "source": [
    "### Insight 1\n",
    "I investigated the distribution and shape of the response variable, `price`, to see whether there are any potential transformations that could be suitable. We can see that the distribution is very right-skewed, so a transformation such as the log transformation would be helpful in using for a predictive regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11eda62",
   "metadata": {},
   "source": [
    "### Insight 2\n",
    "\n",
    "Not only is the response variable, `price`, more suited to be transformed, but other predictors being transformed such as `number_of_reviews` can also be helpful for building the regression model. When we visualize the distribution by creating a histogram, we can see that the distribution is right-skewed, so using a log transformation would also help build a more accurate model.\n",
    "\n",
    "By the same vein, other similar variables such as `number_of_reviews_ltm`, `number_of_reviews_l30d`, and `calculated_host_listings_count_private_rooms` also have a similar right-skewed distribution, so they can also be log transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec0ab6",
   "metadata": {},
   "source": [
    "### Insight 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a4ec5",
   "metadata": {},
   "source": [
    "There are some useful categorical variables that seem like they help with predicting `price`. However, a good amount of those have levels within the categorical variable that do not align with the test data. To build a model containing those variables, we need to ensure that the levels of those variables match between the train and test data. This suggests that combining levels within these categories would be helpful, as hyperspecific levels of a category may not yield much insight in the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c90ed",
   "metadata": {},
   "source": [
    "### Insight 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50311db",
   "metadata": {},
   "source": [
    "Another way I went about doing exploratory data analysis is analyzing the overall correlations of the data set, particularly with respsect to `log_price`.  \n",
    "\n",
    "From the correlations, `accommodates`, `beds`, and `bath` variables seem to be the highest correlated predictors with `price`. As such, this provided a good basis for these predictors to be included in the regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5df5b",
   "metadata": {},
   "source": [
    "### Insight 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8eac7",
   "metadata": {},
   "source": [
    "A useful method I used to filter out more predictors to include in the final model was analyzing the correlations with `price` and `log_price`. Variables like `host_acceptance_rate` and `host_response_rate` were not nearly as highly correlated as I thought with price. Therefore, I removed predictors that had really low correlations with the response `log_price`.\n",
    "\n",
    "Furthermore, I also checked the correlations between predictors to see if multicollinearity occured. For instance, `minimum_minimum_nights` was highly correlated with `minimum_nights`, and `availability_90` was highly correlated with `availability_365` so I removed those predictors that were along the same vein. The others like `maximum_minimum_nights` and `maximum_nights_avg_ntm` also happened to have very low correlation, so it helped further narrow down useful predictors to include in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1013e0b",
   "metadata": {},
   "source": [
    "### Insight 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbf34a",
   "metadata": {},
   "source": [
    "To further improve predictive accuracy, I also examined any potential influential points. I checked for any extreme outliers and removed any observations that had a `price` higher than 10000. In addition, I found any of the most influential points and removed a high leverage point that had very high number of `reviews`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d691ac",
   "metadata": {},
   "source": [
    "## A.2) Data cleaning / preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3e1a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "56469c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./regression-problem/train_regression.csv')\n",
    "test = pd.read_csv('./regression-problem/test_regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "d5f69d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize the property types\n",
    "def categorize_property(property_type):\n",
    "    if 'Entire' in property_type:\n",
    "        return 'Entire Home/Apartment'\n",
    "    elif 'Private' in property_type:\n",
    "        return 'Private Room'\n",
    "    elif 'Shared' in property_type:\n",
    "        return 'Shared Accommodation'\n",
    "    elif property_type in ['Room in hotel', 'Room in boutique hotel', 'Boat']:\n",
    "        return 'Specialty Accommodations'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "94568a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall function to clean training and test data\n",
    "def clean_data(df):\n",
    "    \n",
    "    # Remove $ from response variable and convert to float in training data\n",
    "    if 'price' in df.columns:\n",
    "        df.price = df.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "    # replace missing values of numeric variables wtih the median\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # replace missing values of categorical variables with the mode \n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n",
    "    \n",
    "    # log transform response variable for training data and drop price\n",
    "    if 'price' in df.columns:\n",
    "        df['log_price'] = np.log(df['price'])\n",
    "    \n",
    "    # replace any 0 values to 1 so that it can go through log transformation\n",
    "    df['beds'] = df['beds'].replace(0, .01)\n",
    "    df['accommodates'] = df['accommodates'].replace(0, .01)\n",
    "    df['number_of_reviews'] = df['number_of_reviews'].replace(0, .01)\n",
    "    df['reviews_per_month'] = df['reviews_per_month'].replace(0, .01)\n",
    "    df['number_of_reviews_ltm'] = df['number_of_reviews_ltm'].replace(0, .01)\n",
    "    df['number_of_reviews_l30d'] = df['number_of_reviews_l30d'].replace(0, .01)\n",
    "    df['host_total_listings_count'] = df['host_total_listings_count'].replace(0, .01)\n",
    "    df['host_listings_count'] = df['host_listings_count'].replace(0, .01)\n",
    "    df['calculated_host_listings_count_private_rooms'] = df['calculated_host_listings_count_private_rooms'].replace(0, .01)\n",
    "    df['calculated_host_listings_count_shared_rooms'] = df['calculated_host_listings_count_shared_rooms'].replace(0, .01)\n",
    "    df['calculated_host_listings_count_entire_homes'] = df['calculated_host_listings_count_entire_homes'].replace(0, .01)\n",
    "    \n",
    "    df['log_beds'] = np.log(df.beds)\n",
    "    df['log_accommodates'] = np.log(df.accommodates)\n",
    "    df['log_reviews'] = np.log(df.number_of_reviews)\n",
    "    df['log_reviews_per_month'] = np.log(df.reviews_per_month)\n",
    "    df['log_reviews_ltm'] = np.log(df.number_of_reviews_ltm)\n",
    "    df['log_reviews_l30d'] = np.log(df.number_of_reviews_l30d)\n",
    "    df['log_host_total_listings_count'] = np.log(df.host_total_listings_count)\n",
    "    df['log_host_listings_count'] = np.log(df.host_listings_count)\n",
    "    df['log_host_listings_count_private_rooms'] = np.log(df.calculated_host_listings_count_private_rooms)\n",
    "    df['log_host_listings_count_shared_rooms'] = np.log(df.calculated_host_listings_count_shared_rooms)\n",
    "    df['log_host_listings_count_entire_homes'] = np.log(df.calculated_host_listings_count_entire_homes)\n",
    "\n",
    "    # calculate the number of days since the host became a host\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "    current_date = dt.now()\n",
    "    df['host_since_days'] = (current_date - df['host_since']).dt.days\n",
    "    \n",
    "    # calculate days since first/last review\n",
    "    df['first_review'] = pd.to_datetime(df['first_review'], errors='coerce')\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "\n",
    "    df['first_review_days'] = (current_date - df['first_review']).dt.days\n",
    "    df['last_review_days'] = (current_date - df['last_review']).dt.days\n",
    "    \n",
    "    # make response_rate and acceptance_rate into numeric dtype\n",
    "    df['host_response_rate'] = df['host_response_rate'].str.rstrip('%').astype('float')\n",
    "    df['host_acceptance_rate'] = df['host_acceptance_rate'].str.rstrip('%').astype('float')\n",
    "    \n",
    "    # subgroup property_type (similar levels as room_type so discard room predictor)\n",
    "    df['property_cats'] = df['property_type'].apply(categorize_property)\n",
    "    \n",
    "    # extract numeric values from the 'bathrooms' column\n",
    "    df['bath_numeric'] = df['bathrooms_text'].str.extract('(\\d+\\.*\\d*)', expand=False).astype(float)\n",
    "\n",
    "    # handle \"Half-bath\" by assigning a numeric value of 0.5\n",
    "    df['bath_numeric'] = df.apply(lambda row: 0.5 if 'half' in row['bathrooms_text'].lower() \\\n",
    "                                  else row['bath_numeric'], axis=1)\n",
    "    \n",
    "    # to binary\n",
    "    df.host_is_superhost = df.host_is_superhost.replace({'t': 1, 'f': 0})\n",
    "    df.host_identity_verified = df.host_identity_verified.replace({'t': 1, 'f': 0})\n",
    "    df.host_has_profile_pic = df.host_has_profile_pic.replace({'t': 1, 'f': 0})\n",
    "    df.has_availability = df.has_availability.replace({'t': 1, 'f': 0})\n",
    "    df.instant_bookable = df.instant_bookable.replace({'t': 1, 'f': 0})\n",
    "\n",
    "    # drop the modified/redundant columns\n",
    "    df.drop(columns = ['host_since', 'first_review', 'last_review', 'property_type', 'bathrooms_text', \\\n",
    "                       'number_of_reviews', 'reviews_per_month', 'number_of_reviews_ltm', \\\n",
    "                       'number_of_reviews_l30d', 'host_total_listings_count', 'host_listings_count', \\\n",
    "                      'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', \\\n",
    "                       'calculated_host_listings_count_entire_homes', 'host_id'], inplace = True)\n",
    "    \n",
    "    # drop predictors that have low corr with log_price and high corr with others to help remove multi-collinearity\n",
    "    df.drop(columns = ['first_review_days', 'last_review_days', 'host_acceptance_rate', 'host_response_rate', \n",
    "                       'availability_60', 'availability_90', 'minimum_minimum_nights', \\\n",
    "                       'maximum_maximum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', \\\n",
    "                       'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "ecc566eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_data(train)\n",
    "clean_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "27a0f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract mean price of each host_location predictor\n",
    "host_loc_avg_prices = train.groupby('host_location')['price'].mean().reset_index()\n",
    "host_loc_avg_prices['avg_host_location_price'] = host_loc_avg_prices['price']\n",
    "train = pd.merge(train, host_loc_avg_prices[['host_location', 'avg_host_location_price']], on='host_location', how='left')\n",
    "\n",
    "# extract mean price of each host_neighbourhood predictor\n",
    "host_neigh_avg_prices = train.groupby('host_neighbourhood')['price'].mean().reset_index()\n",
    "host_neigh_avg_prices['avg_host_neighbourhood_price'] = host_neigh_avg_prices['price']\n",
    "train = pd.merge(train, host_neigh_avg_prices[['host_neighbourhood', 'avg_host_neighbourhood_price']], on='host_neighbourhood', how='left')\n",
    "\n",
    "# extract mean price of each neighbourhood_cleansed predictor\n",
    "neigh_avg_prices = train.groupby('neighbourhood_cleansed')['price'].mean().reset_index()\n",
    "neigh_avg_prices['avg_neighbourhood_price'] = neigh_avg_prices['price']\n",
    "train = pd.merge(train, neigh_avg_prices[['neighbourhood_cleansed', 'avg_neighbourhood_price']], on='neighbourhood_cleansed', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "ea647274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract mean price of each host_location predictor for test data\n",
    "test = pd.merge(test, host_loc_avg_prices[['host_location', 'avg_host_location_price']], on='host_location', how='left')\n",
    "\n",
    "# extract mean price of each host_neighbourhood predictor for test data\n",
    "test = pd.merge(test, host_neigh_avg_prices[['host_neighbourhood', 'avg_host_neighbourhood_price']], on='host_neighbourhood', how='left')\n",
    "\n",
    "# extract mean price of each neighbourhood_cleansed predictor for test data\n",
    "test = pd.merge(test, neigh_avg_prices[['neighbourhood_cleansed', 'avg_neighbourhood_price']], on='neighbourhood_cleansed', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "d0f0b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the categorical predictors that we used right above\n",
    "train = train.drop(columns = ['host_neighbourhood', 'neighbourhood_cleansed', 'host_location'])\n",
    "test = test.drop(columns = ['host_neighbourhood', 'neighbourhood_cleansed', 'host_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "19013fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out extreme outliers\n",
    "train = train[train.price < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "9826e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the most influential point\n",
    "train = train.drop(index = 2850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "14a8cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE the remaining categorical variables\n",
    "host_response_time_dummies = pd.get_dummies(train['host_response_time'], prefix='host_response_time')\n",
    "train = pd.concat([train, host_response_time_dummies], axis = 1)\n",
    "\n",
    "host_response_time_dummies = pd.get_dummies(test['host_response_time'], prefix='host_response_time')\n",
    "test = pd.concat([test, host_response_time_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "71fb8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_verifications_dummies = pd.get_dummies(train['host_verifications'], prefix='host_verifications')\n",
    "train = pd.concat([train, host_verifications_dummies], axis = 1)\n",
    "\n",
    "host_verifications_dummies = pd.get_dummies(test['host_verifications'], prefix='host_verifications')\n",
    "test = pd.concat([test, host_verifications_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "bbcfd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_type_dummies = pd.get_dummies(train['room_type'], prefix='room_type')\n",
    "train = pd.concat([train, room_type_dummies], axis = 1)\n",
    "\n",
    "room_type_dummies = pd.get_dummies(test['room_type'], prefix='room_type')\n",
    "test = pd.concat([test, room_type_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "196b4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_cats_dummies = pd.get_dummies(train['property_cats'], prefix='property_cats')\n",
    "train = pd.concat([train, property_cats_dummies], axis = 1)\n",
    "\n",
    "property_cats_dummies = pd.get_dummies(test['property_cats'], prefix='property_cats')\n",
    "test = pd.concat([test, property_cats_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "3cfee90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns = ['host_response_time', 'host_verifications', 'room_type', 'property_cats'])\n",
    "test = test.drop(columns = ['host_response_time', 'host_verifications', 'room_type', 'property_cats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "17d1859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable spacing\n",
    "train.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)\n",
    "test.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "153c8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values of numeric variables with mean in test from added predictors\n",
    "numeric_columns = test.select_dtypes(include=['number']).columns\n",
    "test[numeric_columns] = test[numeric_columns].apply(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "a1b6bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set response and predictors for scaling\n",
    "y_train = train.log_price\n",
    "X_train = train.drop(columns = ['log_price', 'price', 'id'])\n",
    "X_test = test.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "f426ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly features\n",
    "poly = PolynomialFeatures(degree = 2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "4117895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_scaled = scaler.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58e2ad",
   "metadata": {},
   "source": [
    "## A.3) Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f399b",
   "metadata": {},
   "source": [
    "### Step 1: Transformations\n",
    "As touched upon in the first section, there were some variables that were identified to be more suitable for a log transformation due to having a right-skewed distribution. The variables that ended up getting log transformed were `price`, `number_of_reviews`, and the other review variables.\n",
    "\n",
    "Furthermore, I tested out higher-order polynomial transformation terms to add more flexibility to the model. This was useful for the variables that are pretty highly correlated with `log_price`, and I ended up using the second-order polynomial features when setting up the training and test data sets.\n",
    "\n",
    "Just replacing the transformed log versions of the `review` variables instead of the original non-transformed predictors improved the model, which led me to keep them in the model. The correlations of the log versions also backed this up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e5bca",
   "metadata": {},
   "source": [
    "### Step 2: Variable Selection\n",
    "Given that there is a wide array of predictor variables in the train data, it made sense to first think intuitively about predictor variables that make sense in a model predicting `price`. Firstly, `accommodates` stood out as higher number of accommodates should correspond in an increase in price. These conclusions are also supported by the findings in EDA when correlations were examined. The log versions of `accommodates` and `beds` were also highly correlated with `log_price`, so they were also included in the Ridge regression. I chose to use Ridge regression as there is a decent number of predictors resulting from getting all the two-way interactions betweent the predictors.\n",
    "\n",
    "Initially, intuition and trial and error was the main strategy I used when I went about building the model. For this final model, I decided to use Ridge regression as there were many predictors that had potential relationships with `log_price`. I also used `PolynomialFeatures` to get all the two-way interactions between the variables. Therefore, using Ridge to shrink these coefficients was the method of model building and variable selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc31a3d",
   "metadata": {},
   "source": [
    "### Step 3: Significant Interactions\n",
    "\n",
    "As with the variable selection step, I mainly used intuition and trial and error at first to test significant interactions and whether or not they improved the model. I tested `beds` and `bath_numeric`, as it makes sense for there to be an interaction between the two to impact the price (when there are more beds than baths, when they are even, etc.). Interactions between different types of `review` scores also intuitively made sense, as a listing could have better reviews in some areas over others. Furthermore, `property_cats` (`property_type` in broader categories) and `bath_numeric` (`bathrooms_text` numerized) seemed to be significant. \n",
    "\n",
    "Using `PolynomialFeatures` was a more efficient and thorough way of running all the different two-way interactions between the chosen variables. I ran it with a degree of 2 and put the training set through Ridge regression to scale all the coefficients accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148744c",
   "metadata": {},
   "source": [
    "## A.4) Model equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ebba0",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "00964bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "e14a3f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeCV(alphas=array([1.00000000e-01, 1.20679264e-01, 1.45634848e-01, 1.75751062e-01,\n",
       "       2.12095089e-01, 2.55954792e-01, 3.08884360e-01, 3.72759372e-01,\n",
       "       4.49843267e-01, 5.42867544e-01, 6.55128557e-01, 7.90604321e-01,\n",
       "       9.54095476e-01, 1.15139540e+00, 1.38949549e+00, 1.67683294e+00,\n",
       "       2.02358965e+00, 2.44205309e+00, 2.94705170e+00, 3.55648031e+00,\n",
       "       4.29193426e+00, 5.17947468e+0...\n",
       "       9.10298178e+00, 1.09854114e+01, 1.32571137e+01, 1.59985872e+01,\n",
       "       1.93069773e+01, 2.32995181e+01, 2.81176870e+01, 3.39322177e+01,\n",
       "       4.09491506e+01, 4.94171336e+01, 5.96362332e+01, 7.19685673e+01,\n",
       "       8.68511374e+01, 1.04811313e+02, 1.26485522e+02, 1.52641797e+02,\n",
       "       1.84206997e+02, 2.22299648e+02, 2.68269580e+02, 3.23745754e+02,\n",
       "       3.90693994e+02, 4.71486636e+02, 5.68986603e+02, 6.86648845e+02,\n",
       "       8.28642773e+02, 1.00000000e+03]),\n",
       "        cv=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV(alphas=array([1.00000000e-01, 1.20679264e-01, 1.45634848e-01, 1.75751062e-01,\n",
       "       2.12095089e-01, 2.55954792e-01, 3.08884360e-01, 3.72759372e-01,\n",
       "       4.49843267e-01, 5.42867544e-01, 6.55128557e-01, 7.90604321e-01,\n",
       "       9.54095476e-01, 1.15139540e+00, 1.38949549e+00, 1.67683294e+00,\n",
       "       2.02358965e+00, 2.44205309e+00, 2.94705170e+00, 3.55648031e+00,\n",
       "       4.29193426e+00, 5.17947468e+0...\n",
       "       9.10298178e+00, 1.09854114e+01, 1.32571137e+01, 1.59985872e+01,\n",
       "       1.93069773e+01, 2.32995181e+01, 2.81176870e+01, 3.39322177e+01,\n",
       "       4.09491506e+01, 4.94171336e+01, 5.96362332e+01, 7.19685673e+01,\n",
       "       8.68511374e+01, 1.04811313e+02, 1.26485522e+02, 1.52641797e+02,\n",
       "       1.84206997e+02, 2.22299648e+02, 2.68269580e+02, 3.23745754e+02,\n",
       "       3.90693994e+02, 4.71486636e+02, 5.68986603e+02, 6.86648845e+02,\n",
       "       8.28642773e+02, 1.00000000e+03]),\n",
       "        cv=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeCV(alphas=array([1.00000000e-01, 1.20679264e-01, 1.45634848e-01, 1.75751062e-01,\n",
       "       2.12095089e-01, 2.55954792e-01, 3.08884360e-01, 3.72759372e-01,\n",
       "       4.49843267e-01, 5.42867544e-01, 6.55128557e-01, 7.90604321e-01,\n",
       "       9.54095476e-01, 1.15139540e+00, 1.38949549e+00, 1.67683294e+00,\n",
       "       2.02358965e+00, 2.44205309e+00, 2.94705170e+00, 3.55648031e+00,\n",
       "       4.29193426e+00, 5.17947468e+0...\n",
       "       9.10298178e+00, 1.09854114e+01, 1.32571137e+01, 1.59985872e+01,\n",
       "       1.93069773e+01, 2.32995181e+01, 2.81176870e+01, 3.39322177e+01,\n",
       "       4.09491506e+01, 4.94171336e+01, 5.96362332e+01, 7.19685673e+01,\n",
       "       8.68511374e+01, 1.04811313e+02, 1.26485522e+02, 1.52641797e+02,\n",
       "       1.84206997e+02, 2.22299648e+02, 2.68269580e+02, 3.23745754e+02,\n",
       "       3.90693994e+02, 4.71486636e+02, 5.68986603e+02, 6.86648845e+02,\n",
       "       8.28642773e+02, 1.00000000e+03]),\n",
       "        cv=10)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.logspace(-1,3,50)\n",
    "model = RidgeCV(alphas=alphas, cv=10)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "42a29082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.26957952797244"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "0bc46ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.exp(model.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "ae2b0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = test.id.values\n",
    "predicted = pred\n",
    "submission = pd.DataFrame({'id': id, 'predicted': predicted})\n",
    "submission = submission.reset_index(drop=True)\n",
    "submission.to_csv('regression_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
